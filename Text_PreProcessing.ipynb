{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_PreProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIKpyZQzOjcgakv3xXQ8Ao",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushka-code/Coding-Blocks-ML-DL-/blob/main/Text_PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Basics - Text PreProcessing"
      ],
      "metadata": {
        "id": "_AqhR7VYjVbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words Pipeline\n",
        "\n",
        "*   Get the Data/Corpus\n",
        "*   Tokenisation & Stopwords Removal\n",
        "*   Stemming\n",
        "*   Building a Vocab\n",
        "*   Vectorisation\n",
        "*   Classification\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V3ffRVIOKL59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #natural language toolkit\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "FWT6MBG_93SA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "TTLCKIC9Aq98"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"brown\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpA6orHXA7G4",
        "outputId": "31b03860-bbba-47bc-a999-635ce7ea7967"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(brown.categories())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX0sGM-1GyHd",
        "outputId": "a02d1d83-4bcf-4fef-e628-00196a2e6cb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = brown.sents(categories = 'science_fiction')"
      ],
      "metadata": {
        "id": "YIbxOa6OJAj9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[6]"
      ],
      "metadata": {
        "id": "aZAq4SNCJWmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e600692-c635-453f-925f-a92adaa96b1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he',\n",
              " 'studied',\n",
              " 'them',\n",
              " ',',\n",
              " 'compared',\n",
              " 'them',\n",
              " 'with',\n",
              " 'what',\n",
              " 'he',\n",
              " 'had',\n",
              " 'been',\n",
              " 'taught',\n",
              " 'as',\n",
              " 'a',\n",
              " 'nestling',\n",
              " ',',\n",
              " 'struggling',\n",
              " 'to',\n",
              " 'bridge',\n",
              " 'between',\n",
              " 'languages',\n",
              " ',',\n",
              " 'the',\n",
              " 'one',\n",
              " 'he',\n",
              " 'thought',\n",
              " 'with',\n",
              " 'and',\n",
              " 'the',\n",
              " 'one',\n",
              " 'he',\n",
              " 'was',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'think',\n",
              " 'in',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(data[6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "R2QWYF6gJbRR",
        "outputId": "e6b9415c-408c-46a7-f8d0-6ffdc6e48f66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'he studied them , compared them with what he had been taught as a nestling , struggling to bridge between languages , the one he thought with and the one he was learning to think in .'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization - Breaking the document into sentences or breaking the sentences into words"
      ],
      "metadata": {
        "id": "qUhCl_ml_K4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"I introduce myself as Anushka Bhave. I am a computer engineering student. I love to code.\"\"\"\n",
        "sentence = \"\"\"I am passionate about research in the fields of Deep Learning and NLP\"\"\""
      ],
      "metadata": {
        "id": "yK4xt8-NKOAM"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "iibf-C-9_6QD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xXy-fJABLqP",
        "outputId": "db01088e-66ae-4a85-9117-d4c70ec24798"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(document)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4v2x9sxADA5",
        "outputId": "b68eab80-626d-4d0d-d835-72b447375722"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I introduce myself as Anushka Bhave.', 'I am a computer engineering student.', 'I love to code.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ATPXTC45B3qP",
        "outputId": "c8a75da7-89b9-44d6-9079-a878615d41dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I introduce myself as Anushka Bhave.'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhgo-tS9BWtj",
        "outputId": "a5afb472-cf19-480e-84b2-74f3e41a0eb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'passionate', 'about', 'research', 'in', 'the', 'fields', 'of', 'Deep', 'Learning', 'and', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords Removal - These are words which don't add to the analysis of the sentence or don't carry any weightage"
      ],
      "metadata": {
        "id": "MOVmxGfpD1mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "s7tincfXC-UG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE5pbXCLEka5",
        "outputId": "dbb3a8fe-1ffe-4b1c-f9e5-58eb94f63e03"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Ct7KsPxrEcs1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lo3jSTTEq8E",
        "outputId": "52dc09d2-3cac-4847-8a34-e75ab5b2c795"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'than', 'after', 'what', 'about', 'be', 'ourselves', 'which', 'to', 'against', 'this', 'from', 'are', 'more', 'for', \"don't\", 'any', 'above', 'of', 'did', 'further', 'then', 'our', 'him', 'in', \"hasn't\", 'll', \"won't\", \"weren't\", 'out', 'hers', 'when', 'between', 're', 'that', 'haven', 'these', 'he', 'am', 'where', 've', 'm', 'those', 'your', 'who', \"that'll\", 'has', 'can', 'at', 'there', 'most', \"aren't\", 'you', 'by', 'whom', 'up', 'yours', 'yourself', 'aren', 'so', 'why', 'now', 'we', 'should', 'them', 's', 'but', 'shan', 'on', 'because', 'themselves', 'wouldn', \"you've\", 'both', 'own', 'not', 'just', 'my', 'mustn', \"shouldn't\", 'himself', 'before', 'couldn', 'been', 'd', 'hadn', 'mightn', 'until', 'off', 'won', 'while', 'ours', 'an', \"it's\", \"couldn't\", 'isn', 'i', 'o', 'was', 'having', 'few', 'no', 'a', 'each', 'myself', 'down', 'nor', 'only', \"wasn't\", 'such', \"haven't\", 'or', 'same', \"hadn't\", 'didn', 'again', 'through', \"you're\", 'shouldn', 'other', 'weren', 'were', \"you'll\", \"wouldn't\", 'as', 'below', 'hasn', 'being', 'wasn', 'too', 'it', \"should've\", \"didn't\", 'the', \"doesn't\", 'his', 'all', 'here', 'y', 'her', \"you'd\", 't', 'ain', 'don', 'doesn', 'once', 'doing', 'had', 'they', 'into', 'and', 'over', 'under', 'theirs', 'ma', \"mightn't\", 'if', 'during', 'some', 'herself', 'very', 'is', 'its', \"mustn't\", 'with', 'itself', 'do', 'she', 'how', \"isn't\", 'needn', \"shan't\", 'does', \"she's\", 'their', 'have', 'me', 'will', \"needn't\", 'yourselves'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RemoveStopwords(text, stopwords):\n",
        "  useful_words = [w for w in text if w not in stopwords]\n",
        "  return useful_words\n",
        "\n",
        "sent = \"I don't respect him a lot\".split()\n",
        "RemoveStopwords(sent,sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToFo2EoQFEns",
        "outputId": "383439b1-1883-4090-e851-e2502f7ed04f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'respect', 'lot']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "ryyv-jM4MJcp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My email id is bhaveanushka19@gmail.com and contact number is 9657088983\" \n",
        "tokenizer = RegexpTokenizer('[a-zA-Z@.]+') #regular expression tokenization for for rmeoving numbers \n",
        "useful_text = tokenizer.tokenize(text)\n",
        "print(useful_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ge5jYg2NvL4",
        "outputId": "da556b8d-4ded-41f8-dc9e-80e0e5f87448"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'email', 'id', 'is', 'bhaveanushka', '@gmail.com', 'and', 'contact', 'number', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming/Lemmatization - Breaking down the words to their root word like jumps, jumping, jumped all becomes jump"
      ],
      "metadata": {
        "id": "Xm_A2QHMUUxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et_DJB2uU9hy",
        "outputId": "f5f8c181-e131-4c06-e483-173982ffb9c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wn = WordNetLemmatizer()\n",
        "wn.lemmatize('swimming')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4JhVTrg_UYap",
        "outputId": "6b59e7cc-426b-459a-ae0d-5d133918ab16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'swimming'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "ps.stem('jumping')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IjhhVZzuVFPd",
        "outputId": "9dee88fe-4f67-470b-c28c-ebb1a4503d27"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jump'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('jumps')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tt3t1V1-Wa0k",
        "outputId": "a601fbc6-df64-45f9-d07f-5d62d0913af0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jump'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('loving')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_8JBaPXcWizD",
        "outputId": "5a4fa4e0-d019-48a4-b031-cf952fecb91c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('lovely')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tPc4KMN7Wlyf",
        "outputId": "589e6bdd-d06b-4c46-b8a8-ffe282e6c187"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a Vocab & Vectorisation - Vocab is a list of the unique words present in the corpus. Vectorisation is to convert the sentence into numeric form. It has an index for a unique word and keeps the count of repetitions of that word"
      ],
      "metadata": {
        "id": "UL1nNpdoW2JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [ \n",
        "          'The first Test between India and Sri Lanka which ended in three days in Mohali was a special occasion for former India captain Virat Kohli. He became the 12th Indian cricketer to turn out in 100 Tests for India and also surpassed 8,000 runs in Test cricket on Day 1 of the Test',\n",
        "          'On one side, a feminist movement that led the Me Too movement in Asia; on the other, young men whose resistance to the modest gains made by South Korean women has been exploited by the two main candidates.',\n",
        "          'Food prices have also been pushed up by the war, and are a very real consideration and problem for people in poor countries.',\n",
        "          'Gangubai Kathiawadi’ chronicles Ganga rises to power and fame from a demure small-town girl in Gujarat, to the undisputed queen of kamathipura in Mumbai.'          \n",
        "          ]"
      ],
      "metadata": {
        "id": "yXfBQXh_W1xW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "x5bkWG2QOUCC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "hVZyKFpPRzRO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorised_corpus = cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "vMTrMtBuR2Gg"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save = vectorised_corpus.toarray()"
      ],
      "metadata": {
        "id": "_Lm7ibJkSAzK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = cv.vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wc-2I4zaYx5",
        "outputId": "9e5e1e13-983a-4449-d464-84657ae6fd21"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_first = save[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf0wKT47bDkq",
        "outputId": "ea07eded-e0ab-42b4-8d3e-3eab9822dd6a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 3, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 3, 1, 0, 3, 1, 1, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(save[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb5TTIkuwb8P",
        "outputId": "2dd25388-1dfb-41df-90cb-871ba18916c7"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.inverse_transform?"
      ],
      "metadata": {
        "id": "Uc7ozdqEgiwX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_first = save_first.reshape(1,-1)"
      ],
      "metadata": {
        "id": "FPSLg58xhjz9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = cv.inverse_transform(save_first)\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX5HyzcXgBG-",
        "outputId": "34bd9ce0-976a-4733-91e7-958ad821ea71"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(['000', '100', '12th', 'also', 'and', 'became', 'between',\n",
            "       'captain', 'cricket', 'cricketer', 'day', 'days', 'ended', 'first',\n",
            "       'for', 'former', 'he', 'in', 'india', 'indian', 'kohli', 'lanka',\n",
            "       'mohali', 'occasion', 'of', 'on', 'out', 'runs', 'special', 'sri',\n",
            "       'surpassed', 'test', 'tests', 'the', 'three', 'to', 'turn',\n",
            "       'virat', 'was', 'which'], dtype='<U13')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorisation with Stopword Removal"
      ],
      "metadata": {
        "id": "vtpBZN4YkKzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VectorisationwithStopword(sometext):\n",
        "  words = tokenizer.tokenize(sometext.lower())\n",
        "  words = RemoveStopwords(words,sw)\n",
        "  return words"
      ],
      "metadata": {
        "id": "69Yb2lHrkWBJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VectorisationwithStopword(\"Hi, my name is Anushka Bhave and I am a computer engineering student at VIT Pune\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8eioaqzsxT4",
        "outputId": "cfe4ba1f-e146-4665-da14-07274c5417da"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi',\n",
              " 'name',\n",
              " 'anushka',\n",
              " 'bhave',\n",
              " 'computer',\n",
              " 'engineering',\n",
              " 'student',\n",
              " 'vit',\n",
              " 'pune']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_corp = cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "bUKE6deCtNWm"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(tokenizer=VectorisationwithStopword)"
      ],
      "metadata": {
        "id": "09wl_4Vu35rv"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save1 = new_corp.toarray()\n",
        "saveok = save1[0]"
      ],
      "metadata": {
        "id": "rfPqyrSwv9Jo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(saveok) #the length of the unique vocab words reduced from 98 to 72"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQm26bUwwLDj",
        "outputId": "98b9316d-9df5-438a-e281-31e7a26aceb6"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams, Trigrams and N-grams"
      ],
      "metadata": {
        "id": "ij-TBOlT2dtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"Gangubai Kathiawadi is an excellent movie.\"\n",
        "sentence2 = \"Gangubai Kathiawadi is not a good movie.\"\n",
        "sentence3 = \"Gangubai Kathiawadi is an amazing movie. Beautifully acted, emotion and drama are at the core.\""
      ],
      "metadata": {
        "id": "QySyFhhy0pj6"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [sentence1, sentence2, sentence3]"
      ],
      "metadata": {
        "id": "cmUEED3p8g9f"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHWIqo9ZgH2Q",
        "outputId": "cfb7534e-9f44-4cb6-dc32-71f3d9d780cc"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Gangubai Kathiawadi is an excellent movie.', 'Gangubai Kathiawadi is not a good movie.', 'Gangubai Kathiawadi is an amazing movie. Beautifully acted, emotion and drama are at the core.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv1 = CountVectorizer(ngram_range=(2,2))"
      ],
      "metadata": {
        "id": "YjQ0hUNe3SCK"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Normalization : Assigning weights to the words according to its frequency in the document. More the frequency, less important information is given and it has less weightage, hence."
      ],
      "metadata": {
        "id": "7ADDehujAtfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "1eiJcp3VA7lz"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "BsL5m33DfZ_C"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vc = tfidf.fit_transform(docs).toarray()"
      ],
      "metadata": {
        "id": "LcYqv77Zfhh6"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTfPqdrlggKi",
        "outputId": "3794f695-6ed6-44c3-d311-2ee6fffb8acf"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.44102652 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.57989687 0.34249643\n",
            "  0.         0.34249643 0.34249643 0.34249643 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32052772\n",
            "  0.54270061 0.32052772 0.32052772 0.32052772 0.54270061 0.        ]\n",
            " [0.28899189 0.28899189 0.21978578 0.28899189 0.28899189 0.28899189\n",
            "  0.28899189 0.28899189 0.28899189 0.28899189 0.         0.17068326\n",
            "  0.         0.17068326 0.17068326 0.17068326 0.         0.28899189]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5fnZ3xvgYyo",
        "outputId": "ef52e092-d018-41da-b6e8-aae3dc0a5615"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acted': 0,\n",
              " 'amazing': 1,\n",
              " 'an': 2,\n",
              " 'and': 3,\n",
              " 'are': 4,\n",
              " 'at': 5,\n",
              " 'beautifully': 6,\n",
              " 'core': 7,\n",
              " 'drama': 8,\n",
              " 'emotion': 9,\n",
              " 'excellent': 10,\n",
              " 'gangubai': 11,\n",
              " 'good': 12,\n",
              " 'is': 13,\n",
              " 'kathiawadi': 14,\n",
              " 'movie': 15,\n",
              " 'not': 16,\n",
              " 'the': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    }
  ]
}